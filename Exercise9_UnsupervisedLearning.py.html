
# coding: utf-8

# # Exercise: Dimensionality Reduction and Clustering
# 
# 
# 
# ### Part 1: Dimentionality reduction
# 
# Use PCA to reduce the dimension of the Wisconson breast cancer dataset (from 30 features to 2 principle components), then apply supervised learning (logistic regression) using two principle components only. Steps:
#     
#  (a) Scale the data using StandardScaler (zero mean and unit variance variables), then apply PCA to the scaled training data to get two principle components. Print the shape of the training data before and after PCA is applied. (4 pts)
#     
#  (b) Print the variance explained by each of the two principle components (2 pts)
#     
#  (c) Use the two principle components to fit a logistic regression model with regularization parameter c=10. Find the accuracy. Compare it to the accuracy when all the original features (30 features) are used to fit the logistic regression model. (4 pts)
# 
# 
# 

# In[13]:


get_ipython().run_line_magic('matplotlib', 'inline')
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.datasets import load_breast_cancer
from sklearn.preprocessing import StandardScaler 
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics.cluster import adjusted_rand_score
from sklearn.cluster import KMeans
from sklearn.cluster import AgglomerativeClustering

cancer = load_breast_cancer()
X_train, X_test, Y_train, Y_test = train_test_split(cancer.data,cancer.target,random_state=0)
scaler = StandardScaler().fit(X_train)
X_train_transformed= scaler.transform(X_train)
X_test_transformed= scaler.transform(X_test)

PCAModel = PCA(n_components = 2).fit(X_train_transformed)
print("the shape before pca ",X_train_transformed.shape)
x_train_pca = PCAModel.transform(X_train_transformed)
print("the shape after pca ",x_train_pca.shape)


# In[16]:


print("explained variance of first principle components ", 
      PCAModel.explained_variance_ratio_[0] )
print("explained variance of second principle components ", 
      PCAModel.explained_variance_ratio_[1] )


# In[20]:


x_test_pca = PCAModel.transform(X_test_transformed)
LogRegModel = LogisticRegression(C=10,random_state = 0).fit(X_train_transformed,Y_train)
res = LogRegModel.score(X_test_transformed,Y_test)
print("the accuracy of Logistic Regression when all the original features ", res)
LogRegModel1 = LogisticRegression(C=10,random_state = 0).fit(x_train_pca,Y_train)
res1 = LogRegModel1.score(x_test_pca,Y_test)
print("the accuracy of Logistic Regression when two principle component features ", res1)


# ### Part 2: clustering 
# 
# Assume we do not know the actual labels, and use clustering algorithms on the training samples using the two derived principle components only.
# 
# (a) Apply K-means with random_state=0, and K=2. Use scatter plot to visualize the output of K-Means clustering algorithm. (x-axis is first principle component and y-axis is the second principle component). Also plot the actual labels. (4pt)
#     
# (b) Use the actual labels of Y_train to find the K-means clustering score using the adjusted_rand_score. (2pts)
#     
# (c) Find the adjusted_rand_score if we used agglomerative clustering setting number of clusters to 2, using default linkage (Ward). (4pts)

# In[36]:


kmeans = KMeans(n_clusters=2,random_state=0).fit_predict(x_train_pca)
plt.scatter(x_train_pca[:,0], x_train_pca[:,1], c=kmeans,
            s=50);


# In[37]:


print("The clustering score: ",adjusted_rand_score(kmeans,Y_train))


# In[40]:


agg = AgglomerativeClustering(n_clusters=2).fit_predict(x_train_pca)
print("The agglomerative clustering score: ",adjusted_rand_score(agg,Y_train))

